{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook sets up a fasterRCNN model to detect signatures. \n",
    "It's all written in torch, including the dataset.\n",
    "It uses a custom anchor generator to focus on horizontal bounding boxes. \n",
    "All documents are transformed with a canny edge detector, and resized to 400 x 400.\n",
    "The dataset has some errors, which are corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:22:44.350992Z",
     "iopub.status.busy": "2024-12-11T22:22:44.350616Z",
     "iopub.status.idle": "2024-12-11T22:22:48.846377Z",
     "shell.execute_reply": "2024-12-11T22:22:48.845336Z",
     "shell.execute_reply.started": "2024-12-11T22:22:44.350957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "reproducibility = True # don't random shuffle the dataset to keep a consistent division between test and training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-11T22:22:51.082582Z",
     "iopub.status.busy": "2024-12-11T22:22:51.082220Z",
     "iopub.status.idle": "2024-12-11T22:23:28.325729Z",
     "shell.execute_reply": "2024-12-11T22:23:28.324683Z",
     "shell.execute_reply.started": "2024-12-11T22:22:51.082549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path =  /kaggle/working/data\n",
      "../input/signverod\n",
      "Dataset files moved to: data/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def download_dataset():\n",
    "    # path of the dataset (this notebook was made for kaggle)\n",
    "    download_path = \"../input/signverod\"\n",
    "    # Copy to target path\n",
    "    dataPath = \"data/\"\n",
    "    print(\"Path = \", os.path.realpath(\"data\"))\n",
    "    print(download_path)\n",
    "    if not os.path.exists(dataPath):\n",
    "        shutil.copytree(download_path, dataPath)\n",
    "\n",
    "    print(f\"Dataset files moved to: {dataPath}\")\n",
    "\n",
    "    # Fix and merge dataset. See: https://www.kaggle.com/code/alexhorduz/fixing-signverod-dataset\n",
    "    trainDF = pd.read_csv(dataPath + 'train.csv')\n",
    "    testDF = pd.read_csv(dataPath + 'test.csv')\n",
    "    mapping = pd.read_csv(dataPath + 'image_ids.csv')\n",
    "    trainDF.loc[trainDF.index > 4309, 'image_id'] += 2133\n",
    "    trainDF.loc[trainDF.index > 4309, 'id'] += 4737\n",
    "    trainDF.iloc[4307:4316]\n",
    "    testDF.loc[testDF.index > 809, 'image_id'] += 2133\n",
    "    testDF.loc[testDF.index > 809, 'id'] += 4737\n",
    "    testDF.iloc[806:820]\n",
    "    mapping.loc[mapping.index > 2132, 'id'] += 2133\n",
    "    mapping.iloc[2130:2140]\n",
    "    testIDS = set(testDF['id'])\n",
    "    trainIDS = set(trainDF['id'])\n",
    "    duplicated = testIDS.intersection(trainIDS)\n",
    "    trainDF.loc[trainDF['id'] == 26, :]\n",
    "    testDF.loc[testDF['id'] == 26, :]\n",
    "    data = pd.concat([trainDF, testDF]).drop_duplicates().sort_values(['id'])\n",
    "\n",
    "    # Save the fixed version of the dataset\n",
    "    save_path = \"data/raw/fixed_dataset/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    data.to_csv(save_path + \"full_data.csv\", index=False)\n",
    "    mapping.to_csv(save_path + \"updated_image_ids.csv\", index=False)\n",
    "\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:23:28.327156Z",
     "iopub.status.busy": "2024-12-11T22:23:28.326868Z",
     "iopub.status.idle": "2024-12-11T22:23:29.385574Z",
     "shell.execute_reply": "2024-12-11T22:23:29.384203Z",
     "shell.execute_reply.started": "2024-12-11T22:23:28.327126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir data/resized #folder for canny - resized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:23:29.388280Z",
     "iopub.status.busy": "2024-12-11T22:23:29.387289Z",
     "iopub.status.idle": "2024-12-11T22:23:30.528298Z",
     "shell.execute_reply": "2024-12-11T22:23:30.527115Z",
     "shell.execute_reply.started": "2024-12-11T22:23:29.388229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories.csv\timages\t      raw      test.csv   train.csv\n",
      "image_ids.csv\tlabelmap.txt  resized  tfrecords\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:23:30.530213Z",
     "iopub.status.busy": "2024-12-11T22:23:30.529900Z",
     "iopub.status.idle": "2024-12-11T22:31:48.330515Z",
     "shell.execute_reply": "2024-12-11T22:31:48.329546Z",
     "shell.execute_reply.started": "2024-12-11T22:23:30.530183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2765/2765 [08:17<00:00,  5.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "canny = True\n",
    "\n",
    "for filename in tqdm(os.listdir(\"data/images\")):\n",
    "    if os.path.exists(os.path.join(\"data/resized/\", filename)):\n",
    "        continue\n",
    "    img = Image.open(os.path.join(\"data/images\", filename)).convert('L')\n",
    "\n",
    "    if canny:\n",
    "        image_np = np.array(img)\n",
    "        blurred = cv2.GaussianBlur(image_np, (5, 5), 1.4)\n",
    "        edges = cv2.Canny(blurred, threshold1=100, threshold2=200)\n",
    "        img = Image.fromarray(edges)\n",
    "\n",
    "    img.resize((400, 400)).save(os.path.join(\"data/resized\", filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:31:48.332388Z",
     "iopub.status.busy": "2024-12-11T22:31:48.332061Z",
     "iopub.status.idle": "2024-12-11T22:31:49.647203Z",
     "shell.execute_reply": "2024-12-11T22:31:49.646433Z",
     "shell.execute_reply.started": "2024-12-11T22:31:48.332360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transforms=None):\n",
    "        malformed = 0\n",
    "        dataset = {}\n",
    "        image_dir = \"data/resized/\"\n",
    "        image_info_path = \"data/raw/fixed_dataset/updated_image_ids.csv\"\n",
    "        annotations_path = \"data/raw/fixed_dataset/full_data.csv\"\n",
    "        image_info = pd.read_csv(image_info_path)\n",
    "        annotations = pd.read_csv(annotations_path)\n",
    "        signature_annotations = annotations[annotations['category_id'] == 1]\n",
    "        for _, row in tqdm(signature_annotations.iterrows(), total=len(signature_annotations)):\n",
    "            bbox = ast.literal_eval(row['bbox'])\n",
    "            image_id = row['image_id']\n",
    "            image_info_row = image_info[image_info['id'] == image_id].iloc[0]\n",
    "                \n",
    "            file_name = image_dir+image_info_row['file_name']\n",
    "            img_height = 400\n",
    "            img_width = 400\n",
    "            if image_id not in dataset:\n",
    "                dataset[image_id] = {\"name\":file_name, \"boxes\":[]}\n",
    "            # Calculate bounding box in pixel coordinates\n",
    "            x_min = round(bbox[0] * img_width)\n",
    "            y_min = round(bbox[1] * img_height)\n",
    "            x_max = round((bbox[0] + bbox[2]) * img_width)\n",
    "            y_max = round((bbox[1] + bbox[3]) * img_height)\n",
    "\n",
    "            # Sanity check on bounding boxes\n",
    "            if not bbox[0] + bbox[2] < 1 or not bbox[1] + bbox[3] < 1:\n",
    "                malformed += 1\n",
    "                dataset.pop(image_id)\n",
    "                continue\n",
    "            assert (type(x_min) == type(x_max) == type(y_min) == type(y_max) == int)\n",
    "\n",
    "            if x_min < x_max - 1 or y_min < y_max - 1:\n",
    "                dataset[image_id][\"boxes\"].append((x_min, y_min, x_max, y_max))\n",
    "        self.dataset = [y for x, y in sorted(dataset.items()) if len(y[\"boxes\"]) != 0]\n",
    "        self.dataset = self.dataset[1602:] + self.dataset[:1598]\n",
    "\n",
    "        # Other sanity check\n",
    "        for x in self.dataset:\n",
    "            for bb in x[\"boxes\"]:\n",
    "                for coord in bb:\n",
    "                    assert 0 <= coord <= 400\n",
    "        self.transforms = transforms\n",
    "        print(f\"malformed bounding boxes : {malformed} ({malformed/(len(self.dataset) + malformed) * 100}%)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a tuple (Image, target).\n",
    "        Image is the image as a Tensor, target is a dictionary containing \n",
    "        the labels and the bounding boxes\n",
    "        \"\"\"\n",
    "        img_path = self.dataset[idx]['name']\n",
    "        boxes = self.dataset[idx]['boxes'].copy()\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes_t = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones(len(boxes), dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes_t, \"labels\": labels}\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return F.to_tensor(image), target.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:31:49.657768Z",
     "iopub.status.busy": "2024-12-11T22:31:49.657485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "if reproducibility:\n",
    "    dataset = SignDataset()\n",
    "    train_len = int(0.9 * len(dataset))\n",
    "    test_len = len(dataset) - train_len\n",
    "    train_dataset = Subset(dataset, range(train_len))\n",
    "    test_dataset = Subset(dataset, range(train_len, len(dataset)))\n",
    "else:\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(SignDataset(), (0.95, 0.05))\n",
    "\n",
    "def train():\n",
    "    aspect_ratios = ((1, 0.75, 0.5),) * 5  # Stessi rapporti d'aspetto per tutte le feature maps\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16, 32),   # P2: Piccoli oggetti\n",
    "               (32, 64),   # P3\n",
    "               (64, 128),  # P4\n",
    "               (128, 256), # P5\n",
    "               (256, 350)), # P6: Grandi oggetti, vicino al limite dell'immagine\n",
    "        aspect_ratios=aspect_ratios\n",
    "    )\n",
    "    model = fasterrcnn_resnet50_fpn(rpn_anchor_generator=anchor_generator)\n",
    "    num_classes = 2\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        steps = 8 # batch size is 4 * 8 = 32, but this is divided in 16 small batches, to avoid using too much gpu memory\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        i = 0\n",
    "        for images, targets in tqdm(train_loader, total=len(train_loader)):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            total += len(images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            running_loss += losses.item()\n",
    "            if total%(4*20) == 0:\n",
    "                print(running_loss/total)\n",
    "            \n",
    "            losses.backward()\n",
    "            steps -= 1\n",
    "            # if the batch is full or it's the end of the epoch, optimize\n",
    "            if steps == 0 or (i + 1) == len(train_loader):\n",
    "                steps = 8\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            i += 1\n",
    "        print(f\"Epoch {epoch}, Loss: {running_loss/total}\")    \n",
    "    return model\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf model\n",
    "!mkdir model\n",
    "!ls model\n",
    "PATH = \"model/saved\"\n",
    "torch.save(model, PATH)\n",
    "the_model = torch.load(PATH, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# setup train and test dataset, but don't train the model. Load it from the file instead\n",
    "from torch.utils.data import Subset\n",
    "dataset = SignDataset()\n",
    "train_len = int(0.9 * len(dataset))\n",
    "test_len = len(dataset) - train_len\n",
    "train_dataset = Subset(dataset, range(train_len))\n",
    "test_dataset = Subset(dataset, range(train_len, len(dataset)))\n",
    "the_model = torch.load(\"model/saved\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test phase\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    for images, targets in tqdm(test_loader, total=len(test_loader)):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        total += len(images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        print(\"losses : \", loss_dict[0])\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        running_loss += losses.item()\n",
    "        if total%(4*20) == 0:\n",
    "            print(running_loss/total)\n",
    "print(f\"Train loss: {losses.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# steps = 8 \n",
    "# running_loss = 0.0\n",
    "# total = 0\n",
    "# i = 0\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=4,\n",
    "#             shuffle=True,\n",
    "#             collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# for images, targets in tqdm(train_loader, total=len(train_loader)):\n",
    "#     images = list(image.to(device) for image in images)\n",
    "#     total += len(images)\n",
    "#     # print(\"targets \",targets)\n",
    "#     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#     # print('\\n'.join(map(str, [x[\"boxes\"] for x in targets])))\n",
    "    \n",
    "#     loss_dict = model(images, targets)\n",
    "#     losses = sum(loss for loss in loss_dict.values())\n",
    "#     running_loss += losses.item()\n",
    "#     if total%(4*20) == 0:\n",
    "#         print(running_loss/total)\n",
    "    \n",
    "#     losses.backward()\n",
    "#     steps -= 1\n",
    "#     if steps == 0 or (i + 1) == len(train_loader):\n",
    "#         steps = 8\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#     i += 1    \n",
    "# # print(f\"Epoch {epoch}, Loss: {running_loss/total}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rect_inter(a, b):\n",
    "    inter =  (max(a[0], b[0]), max(a[1], b[1]), min(a[2], b[2]), min(a[3], b[3]))\n",
    "    if inter[0] > inter[2] or inter[1] > inter[3]:\n",
    "        return ()\n",
    "    return inter\n",
    "\n",
    "def area(a):\n",
    "    if len(a) == 4:\n",
    "        return (a[2] - a[0]) * (a[3] - a[1])\n",
    "    return 0\n",
    "# Filters the result based on a minimum score.\n",
    "# Also avoids overlap greater than overlap_thres. \n",
    "# overlap_thres is the overlap coefficient between the two boxes\n",
    "# i.e. area(intersection(a, b)) / min(area(a), area(b))\n",
    "def filter_result(result, thresh=0.75, overlap_thres = 0.5):\n",
    "    boxes = result[\"boxes\"]\n",
    "    scores = result[\"scores\"]\n",
    "    \n",
    "    chosen_boxes = []\n",
    "    chosen_scores = []\n",
    "    result = sorted(zip(boxes, scores), key=lambda x:-x[1])\n",
    "    for box, score in result:\n",
    "        # box = box.to(torch.device(\"cpu\"))\n",
    "        if score < thresh:\n",
    "            break\n",
    "        good = True\n",
    "        for other in chosen_boxes:\n",
    "            overlap = area(rect_inter(box, other)) / min(area(box), area(other))\n",
    "            if overlap > overlap_thres:\n",
    "                good = False\n",
    "                break\n",
    "        if good:\n",
    "            chosen_boxes.append(box)\n",
    "            chosen_scores.append(score)\n",
    "    return chosen_boxes, chosen_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizza in blu le box attese, in rosso quelle trovate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, Image\n",
    "import numpy as np\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "the_model.eval()\n",
    "\n",
    "i = random.randint(1, len(test_dataset))\n",
    "print(i)\n",
    "nparr = test_dataset[i][0].permute(1, 2, 0).numpy()\n",
    "img = Image.fromarray(np.uint8(nparr*255))\n",
    "\n",
    "predict = the_model([test_dataset[i][0].to(device)])[0]\n",
    "\n",
    "\n",
    "img1 = ImageDraw.Draw(img, \"RGBA\")\n",
    "boxes, scores = filter_result(predict, thresh=0.5)\n",
    "for box, score in zip(boxes, scores):\n",
    "    \n",
    "    xa, ya, xb, yb = map(int, box)\n",
    "    img1.rectangle((xa, ya, xb, yb), outline=(255, 0, 0, int(90*score)))\n",
    "\n",
    "for box in test_dataset[i][1][\"boxes\"]:\n",
    "    xa, ya, xb, yb = map(int, box)\n",
    "    img1.rectangle((xa, ya, xb, yb), outline=\"blue\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols=1, figsize=(13, 13))\n",
    "ax.imshow(img)\n",
    "\n",
    "print(test_dataset[i][1:])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1922167,
     "sourceId": 3331901,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
